number = 10
#,repeats = 5
)
# Linear regression
# glm (Generalized linear model)
# Cross-Validated (10 fold)
# RMSE = Root mean square deviation or Root mean square error
# Math.sqrt(Sigma[0 - n](values predicted - values observed)^2)
# MAE = computes the average absolute difference between two numeric vectors
glmFitTime <- train(V8 ~ .,
data = xy,
method = "glm",
preProc = c("center", "scale"),
tuneLength = 10,
trControl = myCvControl)
print(glmFitTime)
summary(glmFitTime)
y_hat = predict(glmFitTime, newdata = x)
summary(y_hat)
mean(100*abs(y_hat-y)/y)
# Your error with linear regression
# Support Vector Machine Radial
svmFitTime <- train(V8 ~ .,
data = xy,
method = "svmRadial",
preProc = c("center", "scale"),
#tuneLength = 10,
trControl = myCvControl)
svmFitTime
summary(svmFitTime)
y_hat = predict(svmFitTime, newdata = x)
mean(100*abs(y_hat-y)/y)
# Your error with support vector regression
# Neural Network
nnFitTime <- train(V8 ~ .,
data = xy,
method = "avNNet",
preProc = c("center", "scale"),
trControl = myCvControl,
#tuneLength = 10,
linout = T,
trace = F,
MaxNWts = 10 * (ncol(xy) + 1) + 10 + 1,
maxit = 100)
nnFitTime
summary(nnFitTime)
y_hat = predict(nnFitTime, newdata = x)
mean(100*abs(y_hat-y)/y)
# Your error with neural networks
# random forest
randomforest <- train(V8 ~ ., data = xy,method = "rf",ntree = 500,trControl = myCvControl
)
randomforest
summary(randomforest)
y_hat = predict(randomforest, newdata = x)
mean(100*abs(y_hat-y)/y)
# You can experiment with other methods, here is where you can find the methods caret supports:
# https://topepo.github.io/caret/available-models.html
# Compare models
resamps <- resamples(list(lm = glmFitTime,
svn = svmFitTime,
nn = nnFitTime,
rf = randomforest))
summary(resamps)
# Now working with the time-series modeling
# t needs to be tried for 3 data sets.
# 1. 15minutes.csv
# 2. day.csv
# 3. hourly.csv
t= read.csv("15minutes.csv",header=FALSE)
head(t)
tSeries = ts(t,freq=140244)
head(tSeries)
plot.ts(tSeries)
#time(tSeries)
#quantile(tSeries)
#plot(decompose(tSeries))
#plot(diff(tSeries))
#ggseasonplot(tSeries)
#adf.test(tSeries)
#acf(tSeries, lag.max = 20)
#install.packages("forecast")
library(forecast)
ar <- Arima(tSeries,order=c(3,1,10))
mean(100*abs(fitted(ar) - tSeries)/tSeries)
#13.50665
# Your Arima error
#res = auto.arima(tSeries, stepwise = F, approximation = F)
#res
#plot(forecast(res, h= 3))
# If you are on linux you can uncomment the following lines to run caret on multiple cores
#install.packages("doMC")
#install.packages("caret")
#library(doMC)
#registerDoMC(4)
library(caret)
setwd("F:/SMU2/Data mining/MLAssignment4/")
getwd()
# xy needs to be tried for 3 data sets.
# 1. 15minutesmatrix.txt
# 2. daymatrix.txt
# 3. hourlymatrix.txt
xy=read.table("F:/SMU2/Data mining/Assignment4/15minutesmatrix.txt",sep=' ',header=F)
y=xy[,8]
head(y)
x=xy[,1:7]
View(xy)
help(train)
xy[V8  ~ .]
xy[V8~.]
setwd("D:/Workspace/r-workspace/MCDA 5580/Assignment3")
setwd("F:\\SMU2\\Data mining\\Assignment3\\")
getwd()
library(arules)
library(plyr)
df_user= read.csv("F:/SMU2/Data mining/Assignment3/Final Submission/temp.csv")
df_user= read.csv("F:/SMU2/Data mining/Assignment3/Final Submission/temp.csv")
View(df_user)
View(df_user)
df_user <- df_user[df_user$InvoiceNo != "0", ]
View(df_user)
df_user = ddply(df_user,c("InvoiceNo"),function(dfl)paste(dfl$Description, collapse = ","))
df_user$InvoiceNo = NULL
write.table(df_user,"Milestones2.csv", quote=FALSE, row.names = FALSE, col.names = FALSE)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
summary(tr)
itemFrequencyPlot(tr, topN=10)
#-------------------------------------------------------------
#supp = 0.03
rules = apriori(tr,parameter = list(supp=0.03,conf=0.5))
#supp = 0.03 (Gives No Rules)
#
#-------------------------------------------------------------
#supp = 0.02
rules = apriori(tr,parameter = list(supp=0.02,conf=0.5))
inspect(rules)
#supp = 0.02 (Gives 17 Rules)
#
#-------------------------------------------------------------
#supp = 0.01
rules = apriori(tr,parameter = list(supp=0.01,conf=0.5))
inspect(rules)
#-------------------------------------------------------------
#supp = 0.03
rules = apriori(tr,parameter = list(supp=0.03,conf=0.5))
#supp = 0.03 (Gives No Rules)
#
#-------------------------------------------------------------
#supp = 0.02
rules = apriori(tr,parameter = list(supp=0.02,conf=0.5))
inspect(rules)
#supp = 0.02 (Gives 17 Rules)
#
#-------------------------------------------------------------
#supp = 0.01
rules = apriori(tr,parameter = list(supp=0.01,conf=0.5))
inspect(rules)
rules.sub = subset(rules, subset = lift > 1 & lift < 10)
inspect(rules.sub)
rules.sub = sort(rules.sub,by='lift')
inspect(rules.sub)
itemsets=unique(generatingItemsets(rules.sub))
itemsets
inspect(itemsets)
#-------------------------------------------------------------
#getting the maximally frequent itemsets
help(apriori)
maxrules = apriori(tr,list(supp=0.02,conf=0.5, target="maximally frequent itemsets"))
maxrules = apriori(tr,list(supp=0.02,conf=0.5, target="maximally frequent itemsets"))
inspect(sort(maxrules))
#-------------------------------------------------------------
#plotting the graph.
#install.packages("arulesViz")
library(arulesViz)
plot(rules.sub[1:5],method = "graph",control = list(type = "items"))
plot(rules.sub[1:23],method = "matrix",control = list(type = "items",reorder))
arulesViz::plotly_arules(rules.sub)
arulesViz::plotly_arules(rules.sub[1:15])
plot(sort(rules.sub,by='lift')[1:23],method = "paracoord",control = list(reorder = TRUE))
plot(sort(rules.sub,by='lift')[1:23],method = "paracoord",control = list(reorder = TRUE))
setwd("F:/SMU2/Data mining/Assignment2")
#-----------------Decision tree----------------------#
#
#       RPART
#
#
library(rpart)
library(pROC)
library(randomForest)
library(caret)
library(ggplot2)
set.seed(123)
carData=read.csv("car.csv")
summary(carData)
# View(carData)
x=carData[,1:6]
y=carData[,7]
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
list_data <- c()
library(rpart)
for (i in seq(0, 1000, 10)){
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=i))
#Pridiction
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy
accuracy = sum(diag(treeCM))/sum(treeCM)
list_data <- c(list_data,accuracy)
}
temp_list_data <- c(seq(0, 1000, 10))
temp_list_data
list_data
plot(y=list_data,x=temp_list_data,
main = "Decision Tree - Accuracy measure with Minsplit [0,1000]",
ylab = "Accuracy",
xlab = "Min Split Value")
#-----------------Decision Tree , choosing minsplit as 140 ---------------------------------#
library(rpart)
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=140))
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=140))
treeCar
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy of the prediction using Decision Tree
accuracy=sum(diag(treeCM))/sum(treeCM)
accuracy
predCar_prob=predict(treeCar,training_set,type="prob")
predCar_prob
roc_1 = roc(training_set[,"shouldBuy"],predCar_prob[,1],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(roc_1)
plot(sens.ci, type="shape", col="lightblue")
## Warning in plot.ci.se(sens.ci, type = "shape", col = "lightblue"): Low
## definition shape.
plot(sens.ci, type="bars")
roc_2 = roc(training_set[,"shouldBuy"],predCar_prob[,2],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
roc_3 = roc(training_set[,"shouldBuy"],predCar_prob[,3],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
roc_4 = roc(training_set[,"shouldBuy"],predCar_prob[,4],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(roc_3)
library(caret)
library(e1071)
control <- trainControl(method="cv", number=10, savePredictions=TRUE)
seed <- 123
metric <- "Accuracy"
set.seed(seed)
rf_default <- train(x,y, method="rpart", metric=metric, control = rpart.control(minsplit = 140),
trControl=control)
print(rf_default)
varImp(rf_default)
varImp(rf_default)
p = ggplot(varImp(rf_default))
p + ggtitle("rpart variable importance in a model")
View(carData)
View(carData)
x=carData[,1:5]
y=carData[,7]
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
list_data <- c()
#-----------------Decision Tree , Evaluating accuracy for different minsplit -------------------#
library(rpart)
for (i in seq(0, 1000, 10)){
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=i))
#Pridiction
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy
accuracy = sum(diag(treeCM))/sum(treeCM)
list_data <- c(list_data,accuracy)
}
temp_list_data <- c(seq(0, 1000, 10))
temp_list_data
list_data
plot(y=list_data,x=temp_list_data,
main = "Decision Tree - Accuracy measure with Minsplit [0,1000]",
ylab = "Accuracy",
xlab = "Min Split Value")
#-----------------Decision Tree , choosing minsplit as 140 ---------------------------------#
library(rpart)
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=140))
treeCar
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy of the prediction using Decision Tree
accuracy=sum(diag(treeCM))/sum(treeCM)
accuracy
predCar_prob=predict(treeCar,training_set,type="prob")
predCar_prob
roc_1 = roc(training_set[,"shouldBuy"],predCar_prob[,1],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
library(caret)
library(e1071)
control <- trainControl(method="cv", number=10, savePredictions=TRUE)
seed <- 123
metric <- "Accuracy"
set.seed(seed)
rf_default <- train(x,y, method="rpart", metric=metric, control = rpart.control(minsplit = 140),
trControl=control)
print(rf_default)
varImp(rf_default)
p = ggplot(varImp(rf_default))
p + ggtitle("rpart variable importance in a model")
library(rpart)
library(pROC)
library(randomForest)
library(caret)
carData=read.csv("car.csv")
summary(carData)
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
x=training_set[,1:6]
y=training_set[,7]
list_data <- c()
#-------------Random Forest optimal ntree-------------#
library(randomForest)
for (i in seq(100, 700, 10)){
rf=randomForest(x,y, ntree=i)
rfp=predict(rf,x)
rfCM=table(rfp,y)
accuracy = sum(diag(rfCM))/sum(rfCM)
list_data <- c(list_data,accuracy)
}
temp_list_data <- c(seq(100, 700, 10))
temp_list_data
list_data
plot(y=list_data,x=temp_list_data,
main = "Random forest - Accuracy measure with ntree [100,700]",
ylab = "Accuracy",
xlab = "ntree")
x=training_set[,1:6]
y=training_set[,7]
rf=randomForest(x,y, ntree=260)
rfp=predict(rf,test_set[,1:6])
rfCM=table(rfp,test_set[,7])
rfCM
#Accuracy of the prediction using randomForest
accuracy = sum(diag(rfCM))/sum(rfCM)
accuracy
#------------Random Forest ROC curves ---------------#
rf_prob = predict(rf,x,type="prob")
roc_1 = roc(training_set[,"shouldBuy"],rf_prob[,1],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(roc_1)
plot(sens.ci, type="shape", col="lightblue")
roc_2 = roc(training_set[,"shouldBuy"],rf_prob[,2],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
roc_3 = roc(training_set[,"shouldBuy"],rf_prob[,3],smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.9, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE)
x=carData[,1:6]
y=carData[,7]
# Use caret package for 10 fold cross validation
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 123
metric <- "Accuracy"
set.seed(seed)
rf_default <- train(x,y, method="rf",ntree = 260, metric=metric, trControl=control)
print(rf_default)
varImp(rf_default)
ggplot(varImp(rf_default))+ggtitle("Random Forest variable importance in a model")
x=carData[,1:5]
y=carData[,7]
# Use caret package for 10 fold cross validation
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 123
metric <- "Accuracy"
set.seed(seed)
rf_default <- train(x,y, method="rf",ntree = 260, metric=metric, trControl=control)
print(rf_default)
varImp(rf_default)
ggplot(varImp(rf_default))+ggtitle("Random Forest variable importance in a model")
library(ggplot2)
library(GGally)
library(DMwR)
set.seed(5580)
#Original Data
prod = read.csv('F:/SMU2/Data mining/Assignment 1/Assignment1-Allen/Assignment1/productcluster.csv')
View(prod)
ggpairs(prod[,which(names(prod)!="StockCode")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Products Before Outlier Removal")
prod.clean <- prod[prod$StockCode != "47556B", ]
ggpairs(prod.clean[,which(names(prod.clean)!="StockCode")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Products After Outlier Removal")
prod.scale = scale(prod.clean[-1])
prod.scale
withinSSrange <- function(data,low,high,maxIter)
{
withinss = array(0, dim=c(high-low+1));
for(i in low:high)
{
withinss[i-low+1] <- kmeans(data, i, maxIter)$tot.withinss
}
withinss
}
plot(withinSSrange(prod.scale,1,50,150))
pkm = kmeans(prod.scale, 6, 150)
pkm$centers
prod.realCenters = unscale(pkm$centers, prod.scale)
prod.realCenters
#View(clusteredProd)
plot(clusteredProd[,2:5], col=pkm$cluster)
write.csv(clusteredProd, file ='productcluster1.csv',col.names = FALSE)
prod = read.csv('productcluster1.csv')
library(ggplot2)
library(GGally)
library(DMwR)
set.seed(5580)
#Original Data
cust = read.csv('F:/SMU2/Data mining/Assignment 1/Assignment1-Allen/Assignment1/customercluster.csv')
View(cust)
pkm = kmeans(cust.scale, 10, 150)
cust.scale = scale(cust.clean[-1])
pkm = kmeans(cust.scale, 10, 150)
#Clean Data - Remove Outliers
cust.clean = cust
cust.scale = scale(cust.clean[-1])
pkm = kmeans(cust.scale, 10, 150)
pkm
cust.realCenters = unscale(pkm$centers, cust.scale)
pkm = kmeans(cust.scale, 10, 150)
cust.realCenters = unscale(pkm$centers, cust.scale)
clusteredCust = cbind(cust.clean, pkm$cluster)
cust.realCenters
pkm
pkm$size
pkm$centers
cust.realCenters
pkm$size
pkm$withinss
library(arules)
library(plyr)
df_user= read.csv("F:/SMU2/Data mining/Assignment3/Final Submission/temp.csv")
df_user <- df_user[df_user$InvoiceNo != "0", ]
View(df_user)
df_user = ddply(df_user,c("InvoiceNo"),function(dfl)paste(dfl$Description, collapse = ","))
df_user$InvoiceNo = NULL
write.table(df_user,"Milestones2.csv", quote=FALSE, row.names = FALSE, col.names = FALSE)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
#supp = 0.03
rules = apriori(tr,parameter = list(supp=0.03,conf=0.45))
inspect(rules)
inspect(rules)
rules
#-------------------------------------------------------------
#supp = 0.03
rules = apriori(tr,parameter = list(supp=0.03,conf=0.4))
inspect(rules)
rules
#-------------------------------------------------------------
#supp = 0.03
rules = apriori(tr,parameter = list(supp=0.02,conf=0.45))
inspect(rules)
